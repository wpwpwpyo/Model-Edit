# 模型编辑
## 基于外部存储
【KAFT】Large Language Models with Controllable Working Memory
【MemPrompt】Memory-assisted prompt editing to improve GPT-3 after deployment
【SERAC】Memory-Based Model Editing at Scale
【Language Patch】Fixing Model Bugs with Natural Language Patches
【IKE】Can We Edit Factual Knowledge by In-Context Learning?
【MQuAKE】MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions

【CALINET】Calibrating Factual Knowledge in Pretrained Language Models
【GRACE】Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors
【T-Patcher】Transformer-Patcher: One Mistake worth One Neuron
## 基于全局优化
【RecAdam】Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting
【PPA】Plug-and-Play Adaptation for Continuously-updated QA
【F-Learning】Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models
【Editable Training】Editable Neural Networks
【】Modifying Memories in Transformer Models

【KGEditor】Editing Language Model-based Knowledge Graph Embeddings
【KE】Editing Factual Knowledge in Language Models
【Hyper-Network】HyperNetworks
【SLAG】Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs
【SLAG】Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models
【MEND】Fast Model Editing at Scale
## 基于定位再编辑
【KN】Knowledge Neurons in Pretrained Transformers
【PMET】PMET: Precise Model Editing in a Transformer
【ROME】Locating and Editing Factual Associations in GPT
【MEMIT】Mass-Editing Memory in a Transformer
【DEPN】DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models
【PCGU】Unlearning Bias in Language Models by Partitioning Gradients

【MEMITcsk】Editing Common Sense in Transformers
【BAKE/BIRD】Untying the Reversal Curse via Bidirectional Language Model Editing
